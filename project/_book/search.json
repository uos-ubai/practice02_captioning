[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UBAI PRACTICE - Captioning",
    "section": "",
    "text": "1. 경로 및 환경 설정\n프로젝트 수행에 앞서, 항상 주의해야 할 부분은 경로와 환경입니다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Image Captioning</span>"
    ]
  },
  {
    "objectID": "index.html#경로-및-환경-설정",
    "href": "index.html#경로-및-환경-설정",
    "title": "UBAI PRACTICE - Captioning",
    "section": "",
    "text": "경로 설정\n먼저 임의의 프로젝트 폴더를 생성하겠습니다. 저는 captioning 이라는 폴더를 생성하였습니다. Terminal에서도 해당 경로에서 작업한다는 것을 알려주기 위해, cd 명령어를 통해 작업 폴더(captioning 폴더)로 진입하겠습니다.\n\n📌 참고사항\n명령어 pwd는 현재 작업 경로를 확인하는 코드입니다.\n명령어 cd는 이동하고자하는 작업 경로를 지정하는 코드입니다.\n\n\n\n환경 설정\n다음 프로젝트를 수행하기 위한 가상환경을 생성하겠습니다.\n\nenroot 가상환경\n\n가상환경을 생성하고 활용하는 데에는 여러 방법이 있으나, 그 중 enroot를 활용하여 버전 호환이 이루어진 가상환경을 구축하는 것을 추천드립니다.\n계산 노드에 진입하여 필요한 프레임워크와 OS 버전을 맞춰 enroot를 실행시킵니다.\n이후 계산에 필요한 이미지를 다운받고 컨테이너를 실행시킵니다.\n\n# gpu5 파티션 노드에 진입\nsrun --pty -p gpu5 -c 2 /bin/bash\n\n# dockerhub에서 원하는 이미지를 다운로드\nenroot import docker://eclipse/ubuntu_python\n\n# 다운로드된 이미지 ubuntu.sqsh를 컨테이너로 만듦\nenroot create -n mycontainer eclipse_ubuntu_python.sqsh\n\n# 컨테이너 환경에 진입\nenroot start --root --rw --mount .:/mnt ubuntu-test /bin/bash\n\n\nconda 가상환경\n\n이 외에도 conda를 통해 가상환경을 생성하여 활용하실 수 있습니다.\nconda create -n captioning python=3.8 명령어를 통해 python 버전 3.8을 가진 captioning 이라는 이름의 가상환경을 생성합니다.\n이후 conda activate captioning으로 가상환경에 진입합니다.\n\npip install -r requirements.txt 명령어를 입력하여 필요한 라이브러리를 다운받겠습니다.\n💡 라이브러리를 다운받을 때에는 OS와 라이브러리 등 간의 버전 호환에 반드시 유의하세요! 버전 호환이 되지 않으면 에러가 발생할 수 있습니다!\n\n# requirements.txt\n\ntorch==1.13.1+cu118\ntorchvision==0.14.1+cu118\ntransformers==4.24.0\nmatplotlib==3.5.3\njupyter==1.0.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Image Captioning</span>"
    ]
  },
  {
    "objectID": "index.html#데이터셋-다운로드",
    "href": "index.html#데이터셋-다운로드",
    "title": "UBAI PRACTICE - Captioning",
    "section": "2. 데이터셋 다운로드",
    "text": "2. 데이터셋 다운로드\n캡셔닝 모델을 학습하기 위해 사용할 수 있는 데이터셋은 매우 많습니다. 우리는 그 중 Microsoft COCO (이하 MS COCO) 데이터셋을 활용해보고자 합니다.\nMS COCO는 Object detection(물체 탐지), Segmentation(분류), Captioning에 주로 사용되는 데이터셋으로, 컴퓨터 비전 분야에서 넓은 폭으로 사용되고 있는 데이터셋입니다.\nMS COCO 데이터셋 다운받기 위한 shell 스크립트 파일을 작성하겠습니다.\n\n#!/bin/bash\n\n# COCO dataset directory\nmkdir -p /data/coco\n\n# Download COCO Train2014 images and captions\ncd /data/coco\nwget http://images.cocodataset.org/zips/train2014.zip\nwget http://images.cocodataset.org/zips/val2014.zip\nwget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n\n# Unzip the dataset\nunzip train2014.zip\nunzip val2014.zip\nunzip annotations_trainval2014.zip\n\n✔ mkdir 명령어를 통해 data를 저장하고자 하는 경로를 명시해주세요. mkdir는 해당 경로를 생성해주는 명령어입니다.\n✔ cd 명령어를 통해 생성한 경로로 진입합니다.\n✔ wget 명령어를 통해 MS COCO dataset을 저장할 수 있는 인터넷 사이트로 접속하여, 파일을 다운로드 받습니다.\n✔ unzip 명령어를 통해 저장한 dataset의 zip파일을 압축해제하여, 사용할 수 있는 형태로 둡니다.\n파일 작성이 완료되었다면, 이제 shell 스크립트 파일을 실행해보도록 하겠습니다.\n\n바로 datset_download.sh 명령어를 통해 shell 파일을 실행하다보면, “permission denied (권한 오류)”가 발생할 수 있습니다. 파일 실행 권한을 가지기 위해, chmod 명령어를 사용합니다.\nchmod 명령어는 파일의 권한을 바꿔주는 리눅스 명령어로, 명령어 구성은 다음과 같습니다.\n\nchmod [references][operator][modes] file1 ...\n\n\n\n\noperator\nrole\n\n\n\n\nr\n읽기(read)\n\n\nw\n쓰기(write)\n\n\nx\n실행(execute)\n\n\n\n우리가 실행한 명령어 chmod +x [file_name.sh]는 +x를 통해 실행하는 권한을 [file_name.sh]에 부여한 것입니다.\n\n파일 다운로드가 시작되었습니다.\n모든 파일이 다운로드 되었다면, 반드시 데이터가 정상적으로 다운로드 되었는지 폴더 내 경로로 진입하여 확인하세요.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Image Captioning</span>"
    ]
  },
  {
    "objectID": "index.html#파이썬-프로젝트-실행",
    "href": "index.html#파이썬-프로젝트-실행",
    "title": "UBAI PRACTICE - Captioning",
    "section": "3. 파이썬 프로젝트 실행",
    "text": "3. 파이썬 프로젝트 실행\n모델을 학습시킬 데이터셋을 다운받았으니, 이제 학습할 모델을 지정해주겠습니다.\n여기서는 Transformer 모델을 학습시킵니다. Transformer는 2017년 Google에서 발표된 이후로 딥러닝 전역에서 활발하게 사용되고 있는 모델로, 캡셔닝을 학습하기에도 유용한 모델입니다.\n전체 코드는 transformer.py 파일을 참고하시기 바라며, 링크를 통해 작성한 transformer.py 파일의 세부 코드 및 설명을 확인할 수 있습니다.\n이 파이썬 프로젝트는 slurm을 통해 실행합니다. 이를 위해, HPC 환경에서 모델을 학습하기 위한 slurm 스크립트 transformer.sh를 작성합니다.\n\n#!/bin/bash\n#SBATCH --job-name=captioning\n#SBATCH --output=./output/training_captioning_%n_%j.out\n#SBATCH --error=./output/training_captioning_%n_%j.err\n#SBATCH --nodes=2\n#SBATCH --partition=gpu3\n#SBATCH --gres=gpu:4\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=128G\n#SBATCH --time=24:00:00\n\necho \"start at:\" `date` # 접속한 날짜 표기\necho \"node: $HOSTNAME\" # 접속한 노드 번호 표기\necho \"jobid: $SLURM_JOB_ID\" # jobid 표기\n\n# Load modules\nmodule load cuda/11.8\n\n# Train the transformer-based image captioning model\npython transformer.py\n\n✔ #SBATCH --job-name=captioning job-name을 captioning으로 지정하였습니다.\n✔ output 파일과 error 파일은 output 폴더의 training_captioning이라는 파일명으로 지정하였습니다.\n✔ #SBATCH --nodes=2 복잡한 작업을 요구하는 프로젝트인 만큼, 시간의 효율성을 위해 node 2개를 선택하였습니다. 이에 따라, node 2개로 하나의 작업을 수행하는 병렬 컴퓨팅을 시행합니다.\n✔ #SBATCH --gres=gpu:4 gpu는 4대를 사용하였습니다.\n✔ module load cuda/11.8 module은 cuda 11.8 version을 사용하였습니다.\n✔ python transformer.py transformer.py 파일을 실행합니다.\nsbatch transformer.sh를 통해 작업(job)을 할당하였습니다.\n프로젝트에서 tqdm 라이브러리를 통해 진척도를 확인할 수 있게 세팅해둠에 따라, error 파일에서 진척도를 시각적으로 확인할 수 있습니다.\n\nout 파일에서 학습된 log를 통해, 실행 결과를 확인합니다.\n아래의 그림에 대해 “a shoe rack with some shoes and a dog sleeping on them”과 같은 caption이 생성된 것을 확인할 수 있습니다.\n\n\n이상으로 Captioning 프로젝트 실습을 종료합니다.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Image Captioning</span>"
    ]
  },
  {
    "objectID": "transformer.html",
    "href": "transformer.html",
    "title": "transformer.py",
    "section": "",
    "text": "#########################################\n#### Chapter01. Environment Setting #####\n#########################################\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport math\nfrom tqdm.notebook import trange, tqdm\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision\n\nfrom transformers import AutoTokenizer\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n\n#########################################\n######## Chapter02. Model Define ########\n#########################################\n\n# Define the root directory of the dataset\ndata_set_root='./data/coco'\ntrain_set ='train2014'\nvalidation_set ='val2014'\n\ntrain_image_path = os.path.join(data_set_root, train_set)\ntrain_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, train_set)\n\nval_image_path = os.path.join(data_set_root, validation_set)\nval_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, validation_set)\n\nclass SampleCaption(nn.Module):\n    def __call__(self, sample):\n        rand_index = random.randint(0, len(sample) - 1)\n        return sample[rand_index]\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\nclass TokenDrop(nn.Module):\n\n    def __init__(self, prob=0.1, blank_token=1, eos_token=102):\n        self.prob = prob\n        self.eos_token = eos_token\n        self.blank_token = blank_token\n\n    def __call__(self, sample):\n        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n        can_drop = (~(sample == self.eos_token)).long()\n        mask = mask * can_drop\n        mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n        replace_with = (self.blank_token * torch.ones_like(sample)).long()\n        sample_out = (1 - mask) * sample + mask * replace_with\n        return sample_out\n\ndef extract_patches(image_tensor, patch_size=16):\n    bs, c, h, w = image_tensor.size()\n    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n    unfolded = unfold(image_tensor)\n    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n    return unfolded\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n        super(AttentionBlock, self).__init__()\n        self.masking = masking\n        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True, dropout=0.0)\n\n    def forward(self, x_in, kv_in, key_mask=None):\n        if self.masking:\n            bs, l, h = x_in.shape\n            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n        else:\n            mask = None\n        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask, key_padding_mask=key_mask)[0]\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, hidden_size=128, num_heads=4, decoder=False, masking=True):\n        super(TransformerBlock, self).__init__()\n        self.decoder = decoder\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n        if self.decoder:\n            self.norm2 = nn.LayerNorm(hidden_size)\n            self.attn2 = AttentionBlock(hidden_size=hidden_size,\n                                        num_heads=num_heads, masking=False)\n        self.norm_mlp = nn.LayerNorm(hidden_size)\n        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4), nn.ELU(), nn.Linear(hidden_size * 4, hidden_size))\n\n    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n        x = self.attn1(x, x, key_mask=input_key_mask) + x\n        x = self.norm1(x)\n        if self.decoder:\n            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x\n            x = self.norm2(x)\n        x = self.mlp(x) + x\n        return self.norm_mlp(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(num_emb, hidden_size)\n        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n        self.pos_emb = SinusoidalPosEmb(hidden_size)\n        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=True) for _ in range(num_layers)])\n        self.fc_out = nn.Linear(hidden_size, num_emb)\n\n    def forward(self, input_seq, encoder_output, input_padding_mask=None,\n                encoder_padding_mask=None):\n        input_embs = self.embedding(input_seq)\n        bs, l, h = input_embs.shape\n        seq_indx = torch.arange(l, device=input_seq.device)\n        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n        embs = input_embs + pos_emb\n        for block in self.blocks:\n            embs = block(embs, input_key_mask=input_padding_mask, cross_key_mask=encoder_padding_mask, kv_cross=encoder_output)\n        return self.fc_out(embs)\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128,\n                 num_layers=3, num_heads=4):\n        super(VisionEncoder, self).__init__()\n\n        self.patch_size = patch_size\n        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n\n        seq_length = (image_size // patch_size) ** 2\n        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=False, masking=False) for _ in range(num_layers)])\n    def forward(self, image):\n        bs = image.shape[0]\n        patch_seq = extract_patches(image, patch_size=self.patch_size)\n        patch_emb = self.fc_in(patch_seq)\n        embs = patch_emb + self.pos_embedding\n        for block in self.blocks:\n            embs = block(embs)\n        return embs\n\nclass VisionEncoderDecoder(nn.Module):\n    def __init__(self, image_size, channels_in, num_emb, patch_size=16,\n                 hidden_size=128, num_layers=(3, 3), num_heads=4):\n        super(VisionEncoderDecoder, self).__init__()\n\n        self.encoder = VisionEncoder(image_size=image_size, channels_in=channels_in, patch_size=patch_size, hidden_size=hidden_size, num_layers=num_layers[0], num_heads=num_heads)\n\n        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, num_layers=num_layers[1], num_heads=num_heads)\n\n    def forward(self, input_image, target_seq, padding_mask):\n        bool_padding_mask = padding_mask == 0\n        encoded_seq = self.encoder(image=input_image)\n        decoded_seq = self.decoder(input_seq=target_seq,\n                                   encoder_output=encoded_seq,\n                                   input_padding_mask=bool_padding_mask)\n        return decoded_seq\n\n\n#########################################\n####### Chapter03. Model Training #######\n#########################################\n\n# Define the learning rate for the optimizer\nlearning_rate = 1e-4\n\n# Image size\nimage_size = 128\n\n# Define the number of epochs for training\nnepochs = 3\n\n# Define the batch size for mini-batch gradient descent\nbatch_size = 128\n\n# GPU\ndevice = torch.device(1 if torch.cuda.is_available() else 'cpu')\n\n# Embedding Size\nhidden_size = 192\n\n# Number of Transformer blocks for the (Encoder, Decoder)\nnum_layers = (6, 6)\n\n# MultiheadAttention Heads\nnum_heads = 8\n\n# Size of the patches\npatch_size = 8\n\n# Create model\ncaption_model = VisionEncoderDecoder(image_size=image_size, channels_in=test_images.shape[1], num_emb=tokenizer.vocab_size, patch_size=patch_size, num_layers=num_layers,hidden_size=hidden_size, num_heads=num_heads).to(device)\n\n# Initialize the optimizer with above parameters\noptimizer = optim.Adam(caption_model.parameters(), lr=learning_rate)\nscaler = torch.cuda.amp.GradScaler()\n\n# Define the loss function\nloss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n\ntd = TokenDrop(0.5)\n\n# Initialize the training loss logger\ntraining_loss_logger = []\n\n# Transforms\ntrain_transform = transforms.Compose([transforms.Resize(image_size),\n                                      transforms.RandomCrop(image_size),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n                                      transforms.RandomErasing(p=0.5)])\n\ntransform = transforms.Compose([transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n\ntrain_dataset = datasets.CocoCaptions(root=train_image_path,\n                                      annFile=train_ann_file,\n                                      transform=train_transform,\n                                      target_transform=SampleCaption())\n\nval_dataset = datasets.CocoCaptions(root=val_image_path,\n                                    annFile=val_ann_file,\n                                    transform=transform,\n                                    target_transform=SampleCaption())\n\n# Data Load\ndata_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n\ndataiter = next(iter(data_loader_val))\ntest_images, test_captions = dataiter\n\n# Iterate over epochs\nfor epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n    # Set the model in training mode\n    caption_model.train()\n    steps = 0\n    # Iterate over the training data loader\n    for images, captions in tqdm(data_loader_train, desc=\"Training\", leave=False):\n        images = images.to(device)\n\n        # Tokenize and pre-process the captions\n        tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n        token_ids = tokens['input_ids'].to(device)\n        padding_mask = tokens['attention_mask'].to(device)\n        bs = token_ids.shape[0]\n\n        # Shift the input sequence to create the target sequence\n        target_ids = torch.cat((token_ids[:, 1:],\n                                torch.zeros(bs, 1, device=device).long()), 1)\n        tokens_in = td(token_ids)\n        with torch.cuda.amp.autocast():\n            # Forward pass\n            pred = caption_model(images, tokens_in, padding_mask=padding_mask)\n\n        # Compute the loss\n        loss = (loss_fn(pred.transpose(1, 2), target_ids) * padding_mask).mean()\n\n        # Backpropagation\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Log the training loss\n        training_loss_logger.append(loss.item())\n\n\n#########################################\n###### Chapter04. Model Inference #######\n#########################################\n\n# Create a dataloader itterable object\ndataiter = next(iter(data_loader_val))\n# Sample from the itterable object\ntest_images, test_captions = dataiter\n\n# Choose an index within the batch\nindex = 0\ntest_image = test_images[index].unsqueeze(0)\n\n# Lets visualise an entire batch of images!\nplt.figure(figsize = (3,3))\nout = torchvision.utils.make_grid(test_image, 1, normalize=True)\n_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\nprint(test_captions[index])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>transformer.py</span>"
    ]
  }
]