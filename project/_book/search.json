[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UBAI PRACTICE - Captioning",
    "section": "",
    "text": "1. ê²½ë¡œ ë° í™˜ê²½ ì„¤ì •\ní”„ë¡œì íŠ¸ ìˆ˜í–‰ì— ì•ì„œ, í•­ìƒ ì£¼ì˜í•´ì•¼ í•  ë¶€ë¶„ì€ ê²½ë¡œì™€ í™˜ê²½ì…ë‹ˆë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Image Captioning</span>"
    ]
  },
  {
    "objectID": "index.html#ê²½ë¡œ-ë°-í™˜ê²½-ì„¤ì •",
    "href": "index.html#ê²½ë¡œ-ë°-í™˜ê²½-ì„¤ì •",
    "title": "UBAI PRACTICE - Captioning",
    "section": "",
    "text": "ê²½ë¡œ ì„¤ì •\në¨¼ì € ì„ì˜ì˜ í”„ë¡œì íŠ¸ í´ë”ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤. ì €ëŠ” captioning ì´ë¼ëŠ” í´ë”ë¥¼ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. Terminalì—ì„œë„ í•´ë‹¹ ê²½ë¡œì—ì„œ ì‘ì—…í•œë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ì£¼ê¸° ìœ„í•´, cd ëª…ë ¹ì–´ë¥¼ í†µí•´ ì‘ì—… í´ë”(captioning í´ë”)ë¡œ ì§„ì…í•˜ê² ìŠµë‹ˆë‹¤.\n\nğŸ“Œ ì°¸ê³ ì‚¬í•­\nëª…ë ¹ì–´ pwdëŠ” í˜„ì¬ ì‘ì—… ê²½ë¡œë¥¼ í™•ì¸í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\nëª…ë ¹ì–´ cdëŠ” ì´ë™í•˜ê³ ìí•˜ëŠ” ì‘ì—… ê²½ë¡œë¥¼ ì§€ì •í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\n\n\n\ní™˜ê²½ ì„¤ì •\në‹¤ìŒ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.\n\nenroot ê°€ìƒí™˜ê²½\n\nê°€ìƒí™˜ê²½ì„ ìƒì„±í•˜ê³  í™œìš©í•˜ëŠ” ë°ì—ëŠ” ì—¬ëŸ¬ ë°©ë²•ì´ ìˆìœ¼ë‚˜, ê·¸ ì¤‘ enrootë¥¼ í™œìš©í•˜ì—¬ ë²„ì „ í˜¸í™˜ì´ ì´ë£¨ì–´ì§„ ê°€ìƒí™˜ê²½ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.\nê³„ì‚° ë…¸ë“œì— ì§„ì…í•˜ì—¬ í•„ìš”í•œ í”„ë ˆì„ì›Œí¬ì™€ OS ë²„ì „ì„ ë§ì¶° enrootë¥¼ ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤.\nì´í›„ ê³„ì‚°ì— í•„ìš”í•œ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë°›ê³  ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤.\n\n# gpu5 íŒŒí‹°ì…˜ ë…¸ë“œì— ì§„ì…\nsrun --pty -p gpu5 -c 2 /bin/bash\n\n# dockerhubì—ì„œ ì›í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œ\nenroot import docker://eclipse/ubuntu_python\n\n# ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ ubuntu.sqshë¥¼ ì»¨í…Œì´ë„ˆë¡œ ë§Œë“¦\nenroot create -n mycontainer eclipse_ubuntu_python.sqsh\n\n# ì»¨í…Œì´ë„ˆ í™˜ê²½ì— ì§„ì…\nenroot start --root --rw --mount .:/mnt ubuntu-test /bin/bash\n\n\nconda ê°€ìƒí™˜ê²½\n\nì´ ì™¸ì—ë„ condaë¥¼ í†µí•´ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•˜ì—¬ í™œìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nconda create -n captioning python=3.8 ëª…ë ¹ì–´ë¥¼ í†µí•´ python ë²„ì „ 3.8ì„ ê°€ì§„ captioning ì´ë¼ëŠ” ì´ë¦„ì˜ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤.\nì´í›„ conda activate captioningìœ¼ë¡œ ê°€ìƒí™˜ê²½ì— ì§„ì…í•©ë‹ˆë‹¤.\n\npip install -r requirements.txt ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì—¬ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë‹¤ìš´ë°›ê² ìŠµë‹ˆë‹¤.\nğŸ’¡ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë‹¤ìš´ë°›ì„ ë•Œì—ëŠ” OSì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë“± ê°„ì˜ ë²„ì „ í˜¸í™˜ì— ë°˜ë“œì‹œ ìœ ì˜í•˜ì„¸ìš”! ë²„ì „ í˜¸í™˜ì´ ë˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n\n# requirements.txt\n\ntorch==1.13.1+cu118\ntorchvision==0.14.1+cu118\ntransformers==4.24.0\nmatplotlib==3.5.3\njupyter==1.0.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Image Captioning</span>"
    ]
  },
  {
    "objectID": "index.html#ë°ì´í„°ì…‹-ë‹¤ìš´ë¡œë“œ",
    "href": "index.html#ë°ì´í„°ì…‹-ë‹¤ìš´ë¡œë“œ",
    "title": "UBAI PRACTICE - Captioning",
    "section": "2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ",
    "text": "2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\nìº¡ì…”ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì€ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê·¸ ì¤‘ Microsoft COCO (ì´í•˜ MS COCO) ë°ì´í„°ì…‹ì„ í™œìš©í•´ë³´ê³ ì í•©ë‹ˆë‹¤.\nMS COCOëŠ” Object detection(ë¬¼ì²´ íƒì§€), Segmentation(ë¶„ë¥˜), Captioningì— ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ, ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ë„“ì€ í­ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\nMS COCO ë°ì´í„°ì…‹ ë‹¤ìš´ë°›ê¸° ìœ„í•œ shell ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.\n\n#!/bin/bash\n\n# COCO dataset directory\nmkdir -p /data/coco\n\n# Download COCO Train2014 images and captions\ncd /data/coco\nwget http://images.cocodataset.org/zips/train2014.zip\nwget http://images.cocodataset.org/zips/val2014.zip\nwget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n\n# Unzip the dataset\nunzip train2014.zip\nunzip val2014.zip\nunzip annotations_trainval2014.zip\n\nâœ” mkdir ëª…ë ¹ì–´ë¥¼ í†µí•´ dataë¥¼ ì €ì¥í•˜ê³ ì í•˜ëŠ” ê²½ë¡œë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”. mkdirëŠ” í•´ë‹¹ ê²½ë¡œë¥¼ ìƒì„±í•´ì£¼ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\nâœ” cd ëª…ë ¹ì–´ë¥¼ í†µí•´ ìƒì„±í•œ ê²½ë¡œë¡œ ì§„ì…í•©ë‹ˆë‹¤.\nâœ” wget ëª…ë ¹ì–´ë¥¼ í†µí•´ MS COCO datasetì„ ì €ì¥í•  ìˆ˜ ìˆëŠ” ì¸í„°ë„· ì‚¬ì´íŠ¸ë¡œ ì ‘ì†í•˜ì—¬, íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤.\nâœ” unzip ëª…ë ¹ì–´ë¥¼ í†µí•´ ì €ì¥í•œ datasetì˜ zipíŒŒì¼ì„ ì••ì¶•í•´ì œí•˜ì—¬, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë‘¡ë‹ˆë‹¤.\níŒŒì¼ ì‘ì„±ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´, ì´ì œ shell ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì‹¤í–‰í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\në°”ë¡œ datset_download.sh ëª…ë ¹ì–´ë¥¼ í†µí•´ shell íŒŒì¼ì„ ì‹¤í–‰í•˜ë‹¤ë³´ë©´, â€œpermission denied (ê¶Œí•œ ì˜¤ë¥˜)â€ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŒŒì¼ ì‹¤í–‰ ê¶Œí•œì„ ê°€ì§€ê¸° ìœ„í•´, chmod ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\nchmod ëª…ë ¹ì–´ëŠ” íŒŒì¼ì˜ ê¶Œí•œì„ ë°”ê¿”ì£¼ëŠ” ë¦¬ëˆ…ìŠ¤ ëª…ë ¹ì–´ë¡œ, ëª…ë ¹ì–´ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\nchmod [references][operator][modes] file1 ...\n\n\n\n\noperator\nrole\n\n\n\n\nr\nì½ê¸°(read)\n\n\nw\nì“°ê¸°(write)\n\n\nx\nì‹¤í–‰(execute)\n\n\n\nìš°ë¦¬ê°€ ì‹¤í–‰í•œ ëª…ë ¹ì–´ chmod +x [file_name.sh]ëŠ” +xë¥¼ í†µí•´ ì‹¤í–‰í•˜ëŠ” ê¶Œí•œì„ [file_name.sh]ì— ë¶€ì—¬í•œ ê²ƒì…ë‹ˆë‹¤.\n\níŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\nëª¨ë“  íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆë‹¤ë©´, ë°˜ë“œì‹œ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆëŠ”ì§€ í´ë” ë‚´ ê²½ë¡œë¡œ ì§„ì…í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Image Captioning</span>"
    ]
  },
  {
    "objectID": "index.html#íŒŒì´ì¬-í”„ë¡œì íŠ¸-ì‹¤í–‰",
    "href": "index.html#íŒŒì´ì¬-í”„ë¡œì íŠ¸-ì‹¤í–‰",
    "title": "UBAI PRACTICE - Captioning",
    "section": "3. íŒŒì´ì¬ í”„ë¡œì íŠ¸ ì‹¤í–‰",
    "text": "3. íŒŒì´ì¬ í”„ë¡œì íŠ¸ ì‹¤í–‰\nëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë°›ì•˜ìœ¼ë‹ˆ, ì´ì œ í•™ìŠµí•  ëª¨ë¸ì„ ì§€ì •í•´ì£¼ê² ìŠµë‹ˆë‹¤.\nì—¬ê¸°ì„œëŠ” Transformer ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. TransformerëŠ” 2017ë…„ Googleì—ì„œ ë°œí‘œëœ ì´í›„ë¡œ ë”¥ëŸ¬ë‹ ì „ì—­ì—ì„œ í™œë°œí•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ëª¨ë¸ë¡œ, ìº¡ì…”ë‹ì„ í•™ìŠµí•˜ê¸°ì—ë„ ìœ ìš©í•œ ëª¨ë¸ì…ë‹ˆë‹¤.\nì „ì²´ ì½”ë“œëŠ” transformer.py íŒŒì¼ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ë¼ë©°, ë§í¬ë¥¼ í†µí•´ ì‘ì„±í•œ transformer.py íŒŒì¼ì˜ ì„¸ë¶€ ì½”ë“œ ë° ì„¤ëª…ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ íŒŒì´ì¬ í”„ë¡œì íŠ¸ëŠ” slurmì„ í†µí•´ ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´, HPC í™˜ê²½ì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ slurm ìŠ¤í¬ë¦½íŠ¸ transformer.shë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\n\n#!/bin/bash\n#SBATCH --job-name=captioning\n#SBATCH --output=./output/training_captioning_%n_%j.out\n#SBATCH --error=./output/training_captioning_%n_%j.err\n#SBATCH --nodes=2\n#SBATCH --partition=gpu3\n#SBATCH --gres=gpu:4\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=128G\n#SBATCH --time=24:00:00\n\necho \"start at:\" `date` # ì ‘ì†í•œ ë‚ ì§œ í‘œê¸°\necho \"node: $HOSTNAME\" # ì ‘ì†í•œ ë…¸ë“œ ë²ˆí˜¸ í‘œê¸°\necho \"jobid: $SLURM_JOB_ID\" # jobid í‘œê¸°\n\n# Load modules\nmodule load cuda/11.8\n\n# Train the transformer-based image captioning model\npython transformer.py\n\nâœ” #SBATCH --job-name=captioning job-nameì„ captioningìœ¼ë¡œ ì§€ì •í•˜ì˜€ìŠµë‹ˆë‹¤.\nâœ” output íŒŒì¼ê³¼ error íŒŒì¼ì€ output í´ë”ì˜ training_captioningì´ë¼ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ ì§€ì •í•˜ì˜€ìŠµë‹ˆë‹¤.\nâœ” #SBATCH --nodes=2 ë³µì¡í•œ ì‘ì—…ì„ ìš”êµ¬í•˜ëŠ” í”„ë¡œì íŠ¸ì¸ ë§Œí¼, ì‹œê°„ì˜ íš¨ìœ¨ì„±ì„ ìœ„í•´ node 2ê°œë¥¼ ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì— ë”°ë¼, node 2ê°œë¡œ í•˜ë‚˜ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë³‘ë ¬ ì»´í“¨íŒ…ì„ ì‹œí–‰í•©ë‹ˆë‹¤.\nâœ” #SBATCH --gres=gpu:4 gpuëŠ” 4ëŒ€ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\nâœ” module load cuda/11.8 moduleì€ cuda 11.8 versionì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\nâœ” python transformer.py transformer.py íŒŒì¼ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\nsbatch transformer.shë¥¼ í†µí•´ ì‘ì—…(job)ì„ í• ë‹¹í•˜ì˜€ìŠµë‹ˆë‹¤.\ní”„ë¡œì íŠ¸ì—ì„œ tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì§„ì²™ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê²Œ ì„¸íŒ…í•´ë‘ ì— ë”°ë¼, error íŒŒì¼ì—ì„œ ì§„ì²™ë„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nout íŒŒì¼ì—ì„œ í•™ìŠµëœ logë¥¼ í†µí•´, ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\nì•„ë˜ì˜ ê·¸ë¦¼ì— ëŒ€í•´ â€œa shoe rack with some shoes and a dog sleeping on themâ€ê³¼ ê°™ì€ captionì´ ìƒì„±ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nì´ìƒìœ¼ë¡œ Captioning í”„ë¡œì íŠ¸ ì‹¤ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Image Captioning</span>"
    ]
  },
  {
    "objectID": "transformer.html",
    "href": "transformer.html",
    "title": "transformer.py",
    "section": "",
    "text": "#########################################\n#### Chapter01. Environment Setting #####\n#########################################\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport math\nfrom tqdm.notebook import trange, tqdm\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision\n\nfrom transformers import AutoTokenizer\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n\n#########################################\n######## Chapter02. Model Define ########\n#########################################\n\n# Define the root directory of the dataset\ndata_set_root='./data/coco'\ntrain_set ='train2014'\nvalidation_set ='val2014'\n\ntrain_image_path = os.path.join(data_set_root, train_set)\ntrain_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, train_set)\n\nval_image_path = os.path.join(data_set_root, validation_set)\nval_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, validation_set)\n\nclass SampleCaption(nn.Module):\n    def __call__(self, sample):\n        rand_index = random.randint(0, len(sample) - 1)\n        return sample[rand_index]\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\nclass TokenDrop(nn.Module):\n\n    def __init__(self, prob=0.1, blank_token=1, eos_token=102):\n        self.prob = prob\n        self.eos_token = eos_token\n        self.blank_token = blank_token\n\n    def __call__(self, sample):\n        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n        can_drop = (~(sample == self.eos_token)).long()\n        mask = mask * can_drop\n        mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n        replace_with = (self.blank_token * torch.ones_like(sample)).long()\n        sample_out = (1 - mask) * sample + mask * replace_with\n        return sample_out\n\ndef extract_patches(image_tensor, patch_size=16):\n    bs, c, h, w = image_tensor.size()\n    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n    unfolded = unfold(image_tensor)\n    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n    return unfolded\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n        super(AttentionBlock, self).__init__()\n        self.masking = masking\n        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True, dropout=0.0)\n\n    def forward(self, x_in, kv_in, key_mask=None):\n        if self.masking:\n            bs, l, h = x_in.shape\n            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n        else:\n            mask = None\n        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask, key_padding_mask=key_mask)[0]\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, hidden_size=128, num_heads=4, decoder=False, masking=True):\n        super(TransformerBlock, self).__init__()\n        self.decoder = decoder\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n        if self.decoder:\n            self.norm2 = nn.LayerNorm(hidden_size)\n            self.attn2 = AttentionBlock(hidden_size=hidden_size,\n                                        num_heads=num_heads, masking=False)\n        self.norm_mlp = nn.LayerNorm(hidden_size)\n        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4), nn.ELU(), nn.Linear(hidden_size * 4, hidden_size))\n\n    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n        x = self.attn1(x, x, key_mask=input_key_mask) + x\n        x = self.norm1(x)\n        if self.decoder:\n            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x\n            x = self.norm2(x)\n        x = self.mlp(x) + x\n        return self.norm_mlp(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(num_emb, hidden_size)\n        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n        self.pos_emb = SinusoidalPosEmb(hidden_size)\n        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=True) for _ in range(num_layers)])\n        self.fc_out = nn.Linear(hidden_size, num_emb)\n\n    def forward(self, input_seq, encoder_output, input_padding_mask=None,\n                encoder_padding_mask=None):\n        input_embs = self.embedding(input_seq)\n        bs, l, h = input_embs.shape\n        seq_indx = torch.arange(l, device=input_seq.device)\n        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n        embs = input_embs + pos_emb\n        for block in self.blocks:\n            embs = block(embs, input_key_mask=input_padding_mask, cross_key_mask=encoder_padding_mask, kv_cross=encoder_output)\n        return self.fc_out(embs)\n\nclass VisionEncoder(nn.Module):\n    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128,\n                 num_layers=3, num_heads=4):\n        super(VisionEncoder, self).__init__()\n\n        self.patch_size = patch_size\n        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n\n        seq_length = (image_size // patch_size) ** 2\n        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=False, masking=False) for _ in range(num_layers)])\n    def forward(self, image):\n        bs = image.shape[0]\n        patch_seq = extract_patches(image, patch_size=self.patch_size)\n        patch_emb = self.fc_in(patch_seq)\n        embs = patch_emb + self.pos_embedding\n        for block in self.blocks:\n            embs = block(embs)\n        return embs\n\nclass VisionEncoderDecoder(nn.Module):\n    def __init__(self, image_size, channels_in, num_emb, patch_size=16,\n                 hidden_size=128, num_layers=(3, 3), num_heads=4):\n        super(VisionEncoderDecoder, self).__init__()\n\n        self.encoder = VisionEncoder(image_size=image_size, channels_in=channels_in, patch_size=patch_size, hidden_size=hidden_size, num_layers=num_layers[0], num_heads=num_heads)\n\n        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, num_layers=num_layers[1], num_heads=num_heads)\n\n    def forward(self, input_image, target_seq, padding_mask):\n        bool_padding_mask = padding_mask == 0\n        encoded_seq = self.encoder(image=input_image)\n        decoded_seq = self.decoder(input_seq=target_seq,\n                                   encoder_output=encoded_seq,\n                                   input_padding_mask=bool_padding_mask)\n        return decoded_seq\n\n\n#########################################\n####### Chapter03. Model Training #######\n#########################################\n\n# Define the learning rate for the optimizer\nlearning_rate = 1e-4\n\n# Image size\nimage_size = 128\n\n# Define the number of epochs for training\nnepochs = 3\n\n# Define the batch size for mini-batch gradient descent\nbatch_size = 128\n\n# GPU\ndevice = torch.device(1 if torch.cuda.is_available() else 'cpu')\n\n# Embedding Size\nhidden_size = 192\n\n# Number of Transformer blocks for the (Encoder, Decoder)\nnum_layers = (6, 6)\n\n# MultiheadAttention Heads\nnum_heads = 8\n\n# Size of the patches\npatch_size = 8\n\n# Create model\ncaption_model = VisionEncoderDecoder(image_size=image_size, channels_in=test_images.shape[1], num_emb=tokenizer.vocab_size, patch_size=patch_size, num_layers=num_layers,hidden_size=hidden_size, num_heads=num_heads).to(device)\n\n# Initialize the optimizer with above parameters\noptimizer = optim.Adam(caption_model.parameters(), lr=learning_rate)\nscaler = torch.cuda.amp.GradScaler()\n\n# Define the loss function\nloss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n\ntd = TokenDrop(0.5)\n\n# Initialize the training loss logger\ntraining_loss_logger = []\n\n# Transforms\ntrain_transform = transforms.Compose([transforms.Resize(image_size),\n                                      transforms.RandomCrop(image_size),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n                                      transforms.RandomErasing(p=0.5)])\n\ntransform = transforms.Compose([transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n\ntrain_dataset = datasets.CocoCaptions(root=train_image_path,\n                                      annFile=train_ann_file,\n                                      transform=train_transform,\n                                      target_transform=SampleCaption())\n\nval_dataset = datasets.CocoCaptions(root=val_image_path,\n                                    annFile=val_ann_file,\n                                    transform=transform,\n                                    target_transform=SampleCaption())\n\n# Data Load\ndata_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\ndata_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n\ndataiter = next(iter(data_loader_val))\ntest_images, test_captions = dataiter\n\n# Iterate over epochs\nfor epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n    # Set the model in training mode\n    caption_model.train()\n    steps = 0\n    # Iterate over the training data loader\n    for images, captions in tqdm(data_loader_train, desc=\"Training\", leave=False):\n        images = images.to(device)\n\n        # Tokenize and pre-process the captions\n        tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n        token_ids = tokens['input_ids'].to(device)\n        padding_mask = tokens['attention_mask'].to(device)\n        bs = token_ids.shape[0]\n\n        # Shift the input sequence to create the target sequence\n        target_ids = torch.cat((token_ids[:, 1:],\n                                torch.zeros(bs, 1, device=device).long()), 1)\n        tokens_in = td(token_ids)\n        with torch.cuda.amp.autocast():\n            # Forward pass\n            pred = caption_model(images, tokens_in, padding_mask=padding_mask)\n\n        # Compute the loss\n        loss = (loss_fn(pred.transpose(1, 2), target_ids) * padding_mask).mean()\n\n        # Backpropagation\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Log the training loss\n        training_loss_logger.append(loss.item())\n\n\n#########################################\n###### Chapter04. Model Inference #######\n#########################################\n\n# Create a dataloader itterable object\ndataiter = next(iter(data_loader_val))\n# Sample from the itterable object\ntest_images, test_captions = dataiter\n\n# Choose an index within the batch\nindex = 0\ntest_image = test_images[index].unsqueeze(0)\n\n# Lets visualise an entire batch of images!\nplt.figure(figsize = (3,3))\nout = torchvision.utils.make_grid(test_image, 1, normalize=True)\n_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\nprint(test_captions[index])",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>transformer.py</span>"
    ]
  }
]