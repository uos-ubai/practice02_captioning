{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jsXBgz5GgjOaDBIC2S2XGji1yG9zBxNF",
      "authorship_tag": "ABX9TyMuLztnqVmrqjRRPI9x58KE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/statrav/image_captioning/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이미지 캡셔닝 (Python 프로젝트 실습)\n",
        "\n",
        "Python을 활용한 이미지 캡셔닝 프로젝트를 통해 파일 다운로드부터 파이썬 프로젝트 실행까지의 모든 단계를 다루겠습니다.\n",
        "\n",
        "## 개요\n",
        "이미지 캡셔닝은 주어진 이미지를 설명하는 문장을 생성하는 모델입니다. 예를 들어, 아래의 이미지를 모델에 입력하면 “A black dog sitting among leaves in a forest, surrounded by trees.(검은 개가 숲 속 나무들 사이에서 나뭇잎에 둘러싸여 앉아 있는 모습.)”이라는 캡션을 생성합니다.\n",
        "<img src = \"https://drive.google.com/uc?id=1PIFa73QF1LNQGp_6rjHWR5SAkA4CttVI\" height = 200 width = 500>\n"
      ],
      "metadata": {
        "id": "0BsDDQuL1aQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/jpg/dog.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1F34InmWp1g",
        "outputId": "0278ef1a-8e02-4f84-cae9-21d82f1a6765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jpg/dog.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. 경로 및 환경 설정"
      ],
      "metadata": {
        "id": "sR6c9bp53Vuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 경로 설정\n",
        "프로젝트 수행에 앞서, 경로와 환경 설정이 중요합니다. 먼저 `captioning`이라는 폴더를 생성하고, `cd` 명령어를 통해 해당 폴더로 이동합니다.\n",
        "\n",
        "📌 **참고사항**\n",
        "- `mkdir`: 폴더 생성\n",
        "- `pwd`: 현재 작업 경로 확인\n",
        "- `cd`: 작업 경로 이동"
      ],
      "metadata": {
        "id": "L3wRVZaFB4zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir captioning\n",
        "!cd captioning"
      ],
      "metadata": {
        "id": "NQJbld8U3flm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 환경 설정\n",
        "프로젝트를 위해 필요한 라이브러리를 설치합니다.\n",
        "\n",
        "해당 프로젝트에서는 pytorch 프레임워크와 transformer 모델, 그리고 시각화를 위한 matplotlib 라이브러리를 설치합니다."
      ],
      "metadata": {
        "id": "THh5AyjC3hVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision transformers matplotlib jupyternotebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOX2pXpt2wP7",
        "outputId": "16132491-16d7-4868-fb27-41c1e6bbd861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터셋 다운로드\n",
        "\n",
        "캡셔닝 모델을 학습하기 위해 사용할 수 있는 데이터셋은 매우 많습니다. 우리는 그 중 **Microsoft COCO (이하 MS COCO)** 데이터셋을 활용해보고자 합니다.\n",
        "\n",
        "MS COCO는 Object detection(물체 탐지), Segmentation(분류), Captioning에 주로 사용되는 데이터셋으로, 컴퓨터 비전 분야에서 넓은 폭으로 사용되고 있는 데이터셋입니다.\n",
        "\n",
        "MS COCO 데이터셋 다운받기 위한 shell 스크립트 파일을 작성하겠습니다.\n",
        "\n",
        "📌 **참고사항**\n",
        "\n",
        "- `mkdir`: 폴더 생성\n",
        "- `cd`: 작업 경로 이동\n",
        "- `wget`: 인터넷 주소 접속\n",
        "  - MS COCO dataset을 저장할 수 있는 인터넷 사이트로 접속하여, 파일을 다운로드 받습니다.\n",
        "- `unzip`: 압축 해제"
      ],
      "metadata": {
        "id": "_cu-jwlU4Jym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO dataset directory\n",
        "!mkdir -p /data/coco\n",
        "\n",
        "# Download COCO Train2014 images and captions\n",
        "!cd /data/coco\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip train2014.zip\n",
        "!unzip val2014.zip\n",
        "!unzip annotations_trainval2014.zip"
      ],
      "metadata": {
        "id": "Xy1qyi_D4JNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모든 파일이 다운로드 되었다면, 반드시 데이터가 정상적으로 다운로드 되었는지 폴더 내 경로로 진입하여 확인하세요."
      ],
      "metadata": {
        "id": "fPPiryj-4cNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. 환경 셋팅\n",
        "\n",
        "모델을 학습시킬 데이터셋을 다운받았으니, 이제 학습할 모델을 지정해주겠습니다.\n",
        "\n",
        "여기서는 **Transformer** 모델을 학습시킵니다.\n",
        "\n",
        "📌 Transformer는 2017년 Google에서 발표된 이후로 딥러닝 전역에서 활발하게 사용되고 있는 모델로, 캡셔닝을 학습하기에도 유용한 모델입니다.\n",
        "\n",
        "가장 먼저, 환경 셋팅을 진행하겠습니다.\n",
        "프로젝트에 필요한 라이브러리를 불러오는 과정입니다.\n",
        "\n",
        "또한 위에서 다운받은 데이터에 대한 경로를 지정함으로써, 적절하게 데이터를 불러올 수 있도록 하겠습니다."
      ],
      "metadata": {
        "id": "js64-ZJZgcu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# Define the root directory of the dataset\n",
        "data_set_root='./data/coco'\n",
        "train_set ='train2014'\n",
        "validation_set ='val2014'\n",
        "\n",
        "train_image_path = os.path.join(data_set_root, train_set)\n",
        "train_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, train_set)\n",
        "\n",
        "val_image_path = os.path.join(data_set_root, validation_set)\n",
        "val_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, validation_set)"
      ],
      "metadata": {
        "id": "p4jmpUcPgiE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. 모델 정의\n",
        "\n",
        "모델을 학습하기 전에, 모델을 정의해 줄 필요가 있습니다.\n",
        "학습에 사용될 인코더-디코더 모델을 정의하도록 하겠습니다.\n",
        "\n",
        "아래에서 사용된 모델과 함수들에 대해 간단한 설명을 돕습니다.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "💡 SampleCaption:\n",
        "\n",
        "- sample 리스트에서 무작위로 하나의 요소를 선택하여 반환하는 역할을 합니다.\n",
        "\n",
        "💡 TokenDrop\n",
        "\n",
        "- 토큰 시퀀스에서 무작위로 일부 토큰을 특정 확률에 따라 공백 토큰으로 대체하는 데이터 증강 클래스입니다.\n",
        "\n",
        "- 모델이 특정 정보를 의도적으로 제거하고 학습할 수 있게 합니다.\n",
        "\n",
        "💡 extract_patches\n",
        "\n",
        "- 입력 이미지 텐서를 작은 패치(조각)로 분할하는 함수입니다.\n",
        "\n",
        "💡 SinusoidalPosEmb\n",
        "\n",
        "- 입력 토큰의 위치에 따른 사인 및 코사인 함수 기반의 포지셔널 임베딩을 생성합니다.\n",
        "\n",
        "- 시퀀스 모델에서 순서 정보를 전달하기 위해 사용됩니다.\n",
        "\n",
        "💡 AttentionBlock\n",
        "\n",
        "- 다중 헤드 어텐션 레이어를 정의하는 클래스입니다.\n",
        "\n",
        "💡 TransformerBlock\n",
        "\n",
        "- 인코더와 디코더 블록에서 공통적으로 사용되는 트랜스포머 블록을 정의합니다.\n",
        "\n",
        "- 각 블록은 어텐션 레이어와 MLP(다층 퍼셉트론)로 구성되며, 순차적으로 레이어 정규화와 잔차 연결이 포함됩니다.\n",
        "\n",
        "💡 Decoder\n",
        "\n",
        "- 전체 디코더를 구성하며, 입력 토큰 시퀀스와 인코더 출력값을 받아 출력 시퀀스를 생성합니다.\n",
        "\n",
        "- 임베딩 층과 포지셔널 임베딩, 여러 층의 트랜스포머 블록을 사용하여, 입력 시퀀스의 단어들을 예측하는 디코더 역할을 합니다.\n",
        "\n",
        "💡 VisionEncoder\n",
        "\n",
        "- 이미지를 입력으로 받아 이를 처리하고 시퀀스 형태로 변환하여 임베딩 벡터로 변환합니다.\n",
        "\n",
        "- 이미지의 패치를 추출한 후 선형 변환을 거쳐 인코딩을 수행하고, 포지셔널 임베딩과 트랜스포머 블록을 적용하여 이미지 특징을 추출하는 역할을 합니다.\n",
        "\n",
        "💡 VisionEncoderDecoder\n",
        "\n",
        "- 전체 인코더-디코더 모델을 정의합니다.\n",
        "\n",
        "- VisionEncoder와 Decoder를 각각 인코더와 디코더로 사용하여 이미지 시퀀스를 인코딩한 후, 디코더를 통해 최종 예측값을 생성합니다.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mc15dGaXgwqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleCaption(nn.Module):\n",
        "    def __call__(self, sample):\n",
        "        rand_index = random.randint(0, len(sample) - 1)\n",
        "        return sample[rand_index]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class TokenDrop(nn.Module):\n",
        "\n",
        "    def __init__(self, prob=0.1, blank_token=1, eos_token=102):\n",
        "        self.prob = prob\n",
        "        self.eos_token = eos_token\n",
        "        self.blank_token = blank_token\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
        "        can_drop = (~(sample == self.eos_token)).long()\n",
        "        mask = mask * can_drop\n",
        "        mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n",
        "        replace_with = (self.blank_token * torch.ones_like(sample)).long()\n",
        "        sample_out = (1 - mask) * sample + mask * replace_with\n",
        "        return sample_out\n",
        "\n",
        "def extract_patches(image_tensor, patch_size=16):\n",
        "    bs, c, h, w = image_tensor.size()\n",
        "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
        "    unfolded = unfold(image_tensor)\n",
        "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
        "    return unfolded\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.masking = masking\n",
        "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True, dropout=0.0)\n",
        "\n",
        "    def forward(self, x_in, kv_in, key_mask=None):\n",
        "        if self.masking:\n",
        "            bs, l, h = x_in.shape\n",
        "            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n",
        "        else:\n",
        "            mask = None\n",
        "        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask, key_padding_mask=key_mask)[0]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_size=128, num_heads=4, decoder=False, masking=True):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.decoder = decoder\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n",
        "        if self.decoder:\n",
        "            self.norm2 = nn.LayerNorm(hidden_size)\n",
        "            self.attn2 = AttentionBlock(hidden_size=hidden_size,\n",
        "                                        num_heads=num_heads, masking=False)\n",
        "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
        "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4), nn.ELU(), nn.Linear(hidden_size * 4, hidden_size))\n",
        "\n",
        "    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n",
        "        x = self.attn1(x, x, key_mask=input_key_mask) + x\n",
        "        x = self.norm1(x)\n",
        "        if self.decoder:\n",
        "            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x\n",
        "            x = self.norm2(x)\n",
        "        x = self.mlp(x) + x\n",
        "        return self.norm_mlp(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
        "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
        "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=True) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
        "\n",
        "    def forward(self, input_seq, encoder_output, input_padding_mask=None,\n",
        "                encoder_padding_mask=None):\n",
        "        input_embs = self.embedding(input_seq)\n",
        "        bs, l, h = input_embs.shape\n",
        "        seq_indx = torch.arange(l, device=input_seq.device)\n",
        "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
        "        embs = input_embs + pos_emb\n",
        "        for block in self.blocks:\n",
        "            embs = block(embs, input_key_mask=input_padding_mask, cross_key_mask=encoder_padding_mask, kv_cross=encoder_output)\n",
        "        return self.fc_out(embs)\n",
        "\n",
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128,\n",
        "                 num_layers=3, num_heads=4):\n",
        "        super(VisionEncoder, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
        "\n",
        "        seq_length = (image_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=False, masking=False) for _ in range(num_layers)])\n",
        "    def forward(self, image):\n",
        "        bs = image.shape[0]\n",
        "        patch_seq = extract_patches(image, patch_size=self.patch_size)\n",
        "        patch_emb = self.fc_in(patch_seq)\n",
        "        embs = patch_emb + self.pos_embedding\n",
        "        for block in self.blocks:\n",
        "            embs = block(embs)\n",
        "        return embs\n",
        "\n",
        "class VisionEncoderDecoder(nn.Module):\n",
        "    def __init__(self, image_size, channels_in, num_emb, patch_size=16,\n",
        "                 hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
        "        super(VisionEncoderDecoder, self).__init__()\n",
        "\n",
        "        self.encoder = VisionEncoder(image_size=image_size, channels_in=channels_in, patch_size=patch_size, hidden_size=hidden_size, num_layers=num_layers[0], num_heads=num_heads)\n",
        "\n",
        "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, num_layers=num_layers[1], num_heads=num_heads)\n",
        "\n",
        "    def forward(self, input_image, target_seq, padding_mask):\n",
        "        bool_padding_mask = padding_mask == 0\n",
        "        encoded_seq = self.encoder(image=input_image)\n",
        "        decoded_seq = self.decoder(input_seq=target_seq,\n",
        "                                   encoder_output=encoded_seq,\n",
        "                                   input_padding_mask=bool_padding_mask)\n",
        "        return decoded_seq"
      ],
      "metadata": {
        "id": "i1VE2118g0C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 모델 학습\n",
        "\n",
        "이제 본격적으로 모델 학습을 수행하겠습니다.\n",
        "\n",
        "모델 학습에 필요한 아래의 파라미터(변수)들을 우선 지정합니다.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "🧱 Parameters\n",
        "- Optimizer: 모델의 가중치를 업데이트하여 손실을 최소화하는 알고리즘\n",
        "- Image size: 입력 이미지의 가로와 세로 크기, 보통 픽셀 단위\n",
        "- Epoch: 전체 데이터셋을 한 번 모두 학습하는 과정\n",
        "- Batch size: 한 번의 학습 단계에서 처리하는 데이터 샘플 개수\n",
        "- Hidden size: 트랜스포머 등 모델의 은닉층에서 사용되는 벡터의 차원\n",
        "- Num layers: 모델 내부에 있는 레이어(층)의 개수\n",
        "- Num head: 다중 헤드 어텐션에서 병렬적으로 사용하는 어텐션 헤드의 개수\n",
        "- Patch size: 이미지를 나눌 작은 패치(조각)의 크기\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "이후, 지정한 epoch 수 만큼 학습을 진행하도록 하겠습니다."
      ],
      "metadata": {
        "id": "3QM8l0c_9CRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the learning rate for the optimizer\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Image size\n",
        "image_size = 128\n",
        "\n",
        "# Define the number of epochs for training\n",
        "nepochs = 3\n",
        "\n",
        "# Define the batch size for mini-batch gradient descent\n",
        "batch_size = 128\n",
        "\n",
        "# GPU\n",
        "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Embedding Size\n",
        "hidden_size = 192\n",
        "\n",
        "# Number of Transformer blocks for the (Encoder, Decoder)\n",
        "num_layers = (6, 6)\n",
        "\n",
        "# MultiheadAttention Heads\n",
        "num_heads = 8\n",
        "\n",
        "# Size of the patches\n",
        "patch_size = 8\n",
        "\n",
        "# Create model\n",
        "caption_model = VisionEncoderDecoder(image_size=image_size, channels_in=test_images.shape[1], num_emb=tokenizer.vocab_size, patch_size=patch_size, num_layers=num_layers,hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
        "\n",
        "# Initialize the optimizer with above parameters\n",
        "optimizer = optim.Adam(caption_model.parameters(), lr=learning_rate)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "td = TokenDrop(0.5)\n",
        "\n",
        "# Initialize the training loss logger\n",
        "training_loss_logger = []\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([transforms.Resize(image_size),\n",
        "                                      transforms.RandomCrop(image_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
        "                                      transforms.RandomErasing(p=0.5)])\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(image_size),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "train_dataset = datasets.CocoCaptions(root=train_image_path,\n",
        "                                      annFile=train_ann_file,\n",
        "                                      transform=train_transform,\n",
        "                                      target_transform=SampleCaption())\n",
        "\n",
        "val_dataset = datasets.CocoCaptions(root=val_image_path,\n",
        "                                    annFile=val_ann_file,\n",
        "                                    transform=transform,\n",
        "                                    target_transform=SampleCaption())\n",
        "\n",
        "# Data Load\n",
        "data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "data_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "dataiter = next(iter(data_loader_val))\n",
        "test_images, test_captions = dataiter\n",
        "\n",
        "# Iterate over epochs\n",
        "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n",
        "    # Set the model in training mode\n",
        "    caption_model.train()\n",
        "    steps = 0\n",
        "    # Iterate over the training data loader\n",
        "    for images, captions in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Tokenize and pre-process the captions\n",
        "        tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        token_ids = tokens['input_ids'].to(device)\n",
        "        padding_mask = tokens['attention_mask'].to(device)\n",
        "        bs = token_ids.shape[0]\n",
        "\n",
        "        # Shift the input sequence to create the target sequence\n",
        "        target_ids = torch.cat((token_ids[:, 1:],\n",
        "                                torch.zeros(bs, 1, device=device).long()), 1)\n",
        "        tokens_in = td(token_ids)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # Forward pass\n",
        "            pred = caption_model(images, tokens_in, padding_mask=padding_mask)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = (loss_fn(pred.transpose(1, 2), target_ids) * padding_mask).mean()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Log the training loss\n",
        "        training_loss_logger.append(loss.item())"
      ],
      "metadata": {
        "id": "gmMeu4Ej4bmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 모델 실행\n",
        "\n",
        "모델이 잘 학습되었다면, 실제로 모델을 통해 이미지의 캡션을 생성해보도록 하겠습니다.\n",
        "\n",
        "임의의 한 파일을 통해, 실제로 캡션을 확인해볼까요?\n",
        "\n",
        "여기서는 validation 데이터셋의 이미지로 확인해보도록 하겠습니다.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "🎨 Image\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=177zLaXPriug3w7MHtUcufFN5rEpbYB8A\" height = 200 width = 300>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "위의 이미지에 대해 \"a shoe rack with some shoes and a dog sleeping on them\" 등과 같은 caption이 생성되는 것을 확인할 수 있습니다.\n",
        "\n",
        "이상으로 captioning project를 모두 마치도록 하겠습니다."
      ],
      "metadata": {
        "id": "3mRcZ5a6BTta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dataloader itterable object\n",
        "dataiter = next(iter(data_loader_val))\n",
        "# Sample from the itterable object\n",
        "test_images, test_captions = dataiter\n",
        "\n",
        "# Choose an index within the batch\n",
        "index = 0\n",
        "test_image = test_images[index].unsqueeze(0)\n",
        "\n",
        "# Lets visualise an entire batch of images!\n",
        "plt.figure(figsize = (3,3))\n",
        "out = torchvision.utils.make_grid(test_image, 1, normalize=True)\n",
        "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
        "print(test_captions[index])"
      ],
      "metadata": {
        "id": "mmfKIQs0-hnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}