{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jsXBgz5GgjOaDBIC2S2XGji1yG9zBxNF",
      "authorship_tag": "ABX9TyMuLztnqVmrqjRRPI9x58KE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/statrav/image_captioning/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì´ë¯¸ì§€ ìº¡ì…”ë‹ (Python í”„ë¡œì íŠ¸ ì‹¤ìŠµ)\n",
        "\n",
        "Pythonì„ í™œìš©í•œ ì´ë¯¸ì§€ ìº¡ì…”ë‹ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¶€í„° íŒŒì´ì¬ í”„ë¡œì íŠ¸ ì‹¤í–‰ê¹Œì§€ì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "## ê°œìš”\n",
        "ì´ë¯¸ì§€ ìº¡ì…”ë‹ì€ ì£¼ì–´ì§„ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ë©´ â€œA black dog sitting among leaves in a forest, surrounded by trees.(ê²€ì€ ê°œê°€ ìˆ² ì† ë‚˜ë¬´ë“¤ ì‚¬ì´ì—ì„œ ë‚˜ë­‡ìì— ë‘˜ëŸ¬ì‹¸ì—¬ ì•‰ì•„ ìˆëŠ” ëª¨ìŠµ.)â€ì´ë¼ëŠ” ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "<img src = \"https://drive.google.com/uc?id=1PIFa73QF1LNQGp_6rjHWR5SAkA4CttVI\" height = 200 width = 500>\n"
      ],
      "metadata": {
        "id": "0BsDDQuL1aQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/jpg/dog.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1F34InmWp1g",
        "outputId": "0278ef1a-8e02-4f84-cae9-21d82f1a6765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jpg/dog.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. ê²½ë¡œ ë° í™˜ê²½ ì„¤ì •"
      ],
      "metadata": {
        "id": "sR6c9bp53Vuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 ê²½ë¡œ ì„¤ì •\n",
        "í”„ë¡œì íŠ¸ ìˆ˜í–‰ì— ì•ì„œ, ê²½ë¡œì™€ í™˜ê²½ ì„¤ì •ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë¨¼ì € `captioning`ì´ë¼ëŠ” í´ë”ë¥¼ ìƒì„±í•˜ê³ , `cd` ëª…ë ¹ì–´ë¥¼ í†µí•´ í•´ë‹¹ í´ë”ë¡œ ì´ë™í•©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ **ì°¸ê³ ì‚¬í•­**\n",
        "- `mkdir`: í´ë” ìƒì„±\n",
        "- `pwd`: í˜„ì¬ ì‘ì—… ê²½ë¡œ í™•ì¸\n",
        "- `cd`: ì‘ì—… ê²½ë¡œ ì´ë™"
      ],
      "metadata": {
        "id": "L3wRVZaFB4zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir captioning\n",
        "!cd captioning"
      ],
      "metadata": {
        "id": "NQJbld8U3flm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 í™˜ê²½ ì„¤ì •\n",
        "í”„ë¡œì íŠ¸ë¥¼ ìœ„í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "í•´ë‹¹ í”„ë¡œì íŠ¸ì—ì„œëŠ” pytorch í”„ë ˆì„ì›Œí¬ì™€ transformer ëª¨ë¸, ê·¸ë¦¬ê³  ì‹œê°í™”ë¥¼ ìœ„í•œ matplotlib ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "THh5AyjC3hVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision transformers matplotlib jupyternotebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOX2pXpt2wP7",
        "outputId": "16132491-16d7-4868-fb27-41c1e6bbd861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
        "\n",
        "ìº¡ì…”ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì€ ë§¤ìš° ë§ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ê·¸ ì¤‘ **Microsoft COCO (ì´í•˜ MS COCO)** ë°ì´í„°ì…‹ì„ í™œìš©í•´ë³´ê³ ì í•©ë‹ˆë‹¤.\n",
        "\n",
        "MS COCOëŠ” Object detection(ë¬¼ì²´ íƒì§€), Segmentation(ë¶„ë¥˜), Captioningì— ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ, ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ë„“ì€ í­ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\n",
        "\n",
        "MS COCO ë°ì´í„°ì…‹ ë‹¤ìš´ë°›ê¸° ìœ„í•œ shell ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ **ì°¸ê³ ì‚¬í•­**\n",
        "\n",
        "- `mkdir`: í´ë” ìƒì„±\n",
        "- `cd`: ì‘ì—… ê²½ë¡œ ì´ë™\n",
        "- `wget`: ì¸í„°ë„· ì£¼ì†Œ ì ‘ì†\n",
        "  - MS COCO datasetì„ ì €ì¥í•  ìˆ˜ ìˆëŠ” ì¸í„°ë„· ì‚¬ì´íŠ¸ë¡œ ì ‘ì†í•˜ì—¬, íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤.\n",
        "- `unzip`: ì••ì¶• í•´ì œ"
      ],
      "metadata": {
        "id": "_cu-jwlU4Jym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO dataset directory\n",
        "!mkdir -p /data/coco\n",
        "\n",
        "# Download COCO Train2014 images and captions\n",
        "!cd /data/coco\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip train2014.zip\n",
        "!unzip val2014.zip\n",
        "!unzip annotations_trainval2014.zip"
      ],
      "metadata": {
        "id": "Xy1qyi_D4JNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëª¨ë“  íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆë‹¤ë©´, ë°˜ë“œì‹œ ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ë˜ì—ˆëŠ”ì§€ í´ë” ë‚´ ê²½ë¡œë¡œ ì§„ì…í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "fPPiryj-4cNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. í™˜ê²½ ì…‹íŒ…\n",
        "\n",
        "ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë°›ì•˜ìœ¼ë‹ˆ, ì´ì œ í•™ìŠµí•  ëª¨ë¸ì„ ì§€ì •í•´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì—¬ê¸°ì„œëŠ” **Transformer** ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
        "\n",
        "ğŸ“Œ TransformerëŠ” 2017ë…„ Googleì—ì„œ ë°œí‘œëœ ì´í›„ë¡œ ë”¥ëŸ¬ë‹ ì „ì—­ì—ì„œ í™œë°œí•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ëª¨ë¸ë¡œ, ìº¡ì…”ë‹ì„ í•™ìŠµí•˜ê¸°ì—ë„ ìœ ìš©í•œ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "ê°€ì¥ ë¨¼ì €, í™˜ê²½ ì…‹íŒ…ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "í”„ë¡œì íŠ¸ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "ë˜í•œ ìœ„ì—ì„œ ë‹¤ìš´ë°›ì€ ë°ì´í„°ì— ëŒ€í•œ ê²½ë¡œë¥¼ ì§€ì •í•¨ìœ¼ë¡œì¨, ì ì ˆí•˜ê²Œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "js64-ZJZgcu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# Define the root directory of the dataset\n",
        "data_set_root='./data/coco'\n",
        "train_set ='train2014'\n",
        "validation_set ='val2014'\n",
        "\n",
        "train_image_path = os.path.join(data_set_root, train_set)\n",
        "train_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, train_set)\n",
        "\n",
        "val_image_path = os.path.join(data_set_root, validation_set)\n",
        "val_ann_file = '{}/annotations/captions_{}.json'.format(data_set_root, validation_set)"
      ],
      "metadata": {
        "id": "p4jmpUcPgiE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. ëª¨ë¸ ì •ì˜\n",
        "\n",
        "ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ì „ì—, ëª¨ë¸ì„ ì •ì˜í•´ ì¤„ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.\n",
        "í•™ìŠµì— ì‚¬ìš©ë  ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ì„ ì •ì˜í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ì—ì„œ ì‚¬ìš©ëœ ëª¨ë¸ê³¼ í•¨ìˆ˜ë“¤ì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ ë•ìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ğŸ’¡ SampleCaption:\n",
        "\n",
        "- sample ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ì˜ ìš”ì†Œë¥¼ ì„ íƒí•˜ì—¬ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ’¡ TokenDrop\n",
        "\n",
        "- í† í° ì‹œí€€ìŠ¤ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì¼ë¶€ í† í°ì„ íŠ¹ì • í™•ë¥ ì— ë”°ë¼ ê³µë°± í† í°ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
        "\n",
        "- ëª¨ë¸ì´ íŠ¹ì • ì •ë³´ë¥¼ ì˜ë„ì ìœ¼ë¡œ ì œê±°í•˜ê³  í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ’¡ extract_patches\n",
        "\n",
        "- ì…ë ¥ ì´ë¯¸ì§€ í…ì„œë¥¼ ì‘ì€ íŒ¨ì¹˜(ì¡°ê°)ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ’¡ SinusoidalPosEmb\n",
        "\n",
        "- ì…ë ¥ í† í°ì˜ ìœ„ì¹˜ì— ë”°ë¥¸ ì‚¬ì¸ ë° ì½”ì‚¬ì¸ í•¨ìˆ˜ ê¸°ë°˜ì˜ í¬ì§€ì…”ë„ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ì‹œí€€ìŠ¤ ëª¨ë¸ì—ì„œ ìˆœì„œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ’¡ AttentionBlock\n",
        "\n",
        "- ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ’¡ TransformerBlock\n",
        "\n",
        "- ì¸ì½”ë”ì™€ ë””ì½”ë” ë¸”ë¡ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ê° ë¸”ë¡ì€ ì–´í…ì…˜ ë ˆì´ì–´ì™€ MLP(ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ )ë¡œ êµ¬ì„±ë˜ë©°, ìˆœì°¨ì ìœ¼ë¡œ ë ˆì´ì–´ ì •ê·œí™”ì™€ ì”ì°¨ ì—°ê²°ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ’¡ Decoder\n",
        "\n",
        "- ì „ì²´ ë””ì½”ë”ë¥¼ êµ¬ì„±í•˜ë©°, ì…ë ¥ í† í° ì‹œí€€ìŠ¤ì™€ ì¸ì½”ë” ì¶œë ¥ê°’ì„ ë°›ì•„ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ì„ë² ë”© ì¸µê³¼ í¬ì§€ì…”ë„ ì„ë² ë”©, ì—¬ëŸ¬ ì¸µì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ ì‚¬ìš©í•˜ì—¬, ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡í•˜ëŠ” ë””ì½”ë” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ’¡ VisionEncoder\n",
        "\n",
        "- ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì´ë¥¼ ì²˜ë¦¬í•˜ê³  ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ì´ë¯¸ì§€ì˜ íŒ¨ì¹˜ë¥¼ ì¶”ì¶œí•œ í›„ ì„ í˜• ë³€í™˜ì„ ê±°ì³ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•˜ê³ , í¬ì§€ì…”ë„ ì„ë² ë”©ê³¼ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ ì ìš©í•˜ì—¬ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ğŸ’¡ VisionEncoderDecoder\n",
        "\n",
        "- ì „ì²´ ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "- VisionEncoderì™€ Decoderë¥¼ ê°ê° ì¸ì½”ë”ì™€ ë””ì½”ë”ë¡œ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ë¥¼ ì¸ì½”ë”©í•œ í›„, ë””ì½”ë”ë¥¼ í†µí•´ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mc15dGaXgwqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleCaption(nn.Module):\n",
        "    def __call__(self, sample):\n",
        "        rand_index = random.randint(0, len(sample) - 1)\n",
        "        return sample[rand_index]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class TokenDrop(nn.Module):\n",
        "\n",
        "    def __init__(self, prob=0.1, blank_token=1, eos_token=102):\n",
        "        self.prob = prob\n",
        "        self.eos_token = eos_token\n",
        "        self.blank_token = blank_token\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
        "        can_drop = (~(sample == self.eos_token)).long()\n",
        "        mask = mask * can_drop\n",
        "        mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n",
        "        replace_with = (self.blank_token * torch.ones_like(sample)).long()\n",
        "        sample_out = (1 - mask) * sample + mask * replace_with\n",
        "        return sample_out\n",
        "\n",
        "def extract_patches(image_tensor, patch_size=16):\n",
        "    bs, c, h, w = image_tensor.size()\n",
        "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
        "    unfolded = unfold(image_tensor)\n",
        "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
        "    return unfolded\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.masking = masking\n",
        "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True, dropout=0.0)\n",
        "\n",
        "    def forward(self, x_in, kv_in, key_mask=None):\n",
        "        if self.masking:\n",
        "            bs, l, h = x_in.shape\n",
        "            mask = torch.triu(torch.ones(l, l, device=x_in.device), 1).bool()\n",
        "        else:\n",
        "            mask = None\n",
        "        return self.multihead_attn(x_in, kv_in, kv_in, attn_mask=mask, key_padding_mask=key_mask)[0]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_size=128, num_heads=4, decoder=False, masking=True):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.decoder = decoder\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.attn1 = AttentionBlock(hidden_size=hidden_size, num_heads=num_heads, masking=masking)\n",
        "        if self.decoder:\n",
        "            self.norm2 = nn.LayerNorm(hidden_size)\n",
        "            self.attn2 = AttentionBlock(hidden_size=hidden_size,\n",
        "                                        num_heads=num_heads, masking=False)\n",
        "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
        "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size * 4), nn.ELU(), nn.Linear(hidden_size * 4, hidden_size))\n",
        "\n",
        "    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n",
        "        x = self.attn1(x, x, key_mask=input_key_mask) + x\n",
        "        x = self.norm1(x)\n",
        "        if self.decoder:\n",
        "            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x\n",
        "            x = self.norm2(x)\n",
        "        x = self.mlp(x) + x\n",
        "        return self.norm_mlp(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
        "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
        "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=True) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
        "\n",
        "    def forward(self, input_seq, encoder_output, input_padding_mask=None,\n",
        "                encoder_padding_mask=None):\n",
        "        input_embs = self.embedding(input_seq)\n",
        "        bs, l, h = input_embs.shape\n",
        "        seq_indx = torch.arange(l, device=input_seq.device)\n",
        "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
        "        embs = input_embs + pos_emb\n",
        "        for block in self.blocks:\n",
        "            embs = block(embs, input_key_mask=input_padding_mask, cross_key_mask=encoder_padding_mask, kv_cross=encoder_output)\n",
        "        return self.fc_out(embs)\n",
        "\n",
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128,\n",
        "                 num_layers=3, num_heads=4):\n",
        "        super(VisionEncoder, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
        "\n",
        "        seq_length = (image_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=False, masking=False) for _ in range(num_layers)])\n",
        "    def forward(self, image):\n",
        "        bs = image.shape[0]\n",
        "        patch_seq = extract_patches(image, patch_size=self.patch_size)\n",
        "        patch_emb = self.fc_in(patch_seq)\n",
        "        embs = patch_emb + self.pos_embedding\n",
        "        for block in self.blocks:\n",
        "            embs = block(embs)\n",
        "        return embs\n",
        "\n",
        "class VisionEncoderDecoder(nn.Module):\n",
        "    def __init__(self, image_size, channels_in, num_emb, patch_size=16,\n",
        "                 hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
        "        super(VisionEncoderDecoder, self).__init__()\n",
        "\n",
        "        self.encoder = VisionEncoder(image_size=image_size, channels_in=channels_in, patch_size=patch_size, hidden_size=hidden_size, num_layers=num_layers[0], num_heads=num_heads)\n",
        "\n",
        "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, num_layers=num_layers[1], num_heads=num_heads)\n",
        "\n",
        "    def forward(self, input_image, target_seq, padding_mask):\n",
        "        bool_padding_mask = padding_mask == 0\n",
        "        encoded_seq = self.encoder(image=input_image)\n",
        "        decoded_seq = self.decoder(input_seq=target_seq,\n",
        "                                   encoder_output=encoded_seq,\n",
        "                                   input_padding_mask=bool_padding_mask)\n",
        "        return decoded_seq"
      ],
      "metadata": {
        "id": "i1VE2118g0C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµì„ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ì•„ë˜ì˜ íŒŒë¼ë¯¸í„°(ë³€ìˆ˜)ë“¤ì„ ìš°ì„  ì§€ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ğŸ§± Parameters\n",
        "- Optimizer: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜\n",
        "- Image size: ì…ë ¥ ì´ë¯¸ì§€ì˜ ê°€ë¡œì™€ ì„¸ë¡œ í¬ê¸°, ë³´í†µ í”½ì…€ ë‹¨ìœ„\n",
        "- Epoch: ì „ì²´ ë°ì´í„°ì…‹ì„ í•œ ë²ˆ ëª¨ë‘ í•™ìŠµí•˜ëŠ” ê³¼ì •\n",
        "- Batch size: í•œ ë²ˆì˜ í•™ìŠµ ë‹¨ê³„ì—ì„œ ì²˜ë¦¬í•˜ëŠ” ë°ì´í„° ìƒ˜í”Œ ê°œìˆ˜\n",
        "- Hidden size: íŠ¸ëœìŠ¤í¬ë¨¸ ë“± ëª¨ë¸ì˜ ì€ë‹‰ì¸µì—ì„œ ì‚¬ìš©ë˜ëŠ” ë²¡í„°ì˜ ì°¨ì›\n",
        "- Num layers: ëª¨ë¸ ë‚´ë¶€ì— ìˆëŠ” ë ˆì´ì–´(ì¸µ)ì˜ ê°œìˆ˜\n",
        "- Num head: ë‹¤ì¤‘ í—¤ë“œ ì–´í…ì…˜ì—ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì–´í…ì…˜ í—¤ë“œì˜ ê°œìˆ˜\n",
        "- Patch size: ì´ë¯¸ì§€ë¥¼ ë‚˜ëˆŒ ì‘ì€ íŒ¨ì¹˜(ì¡°ê°)ì˜ í¬ê¸°\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ì´í›„, ì§€ì •í•œ epoch ìˆ˜ ë§Œí¼ í•™ìŠµì„ ì§„í–‰í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "3QM8l0c_9CRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the learning rate for the optimizer\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Image size\n",
        "image_size = 128\n",
        "\n",
        "# Define the number of epochs for training\n",
        "nepochs = 3\n",
        "\n",
        "# Define the batch size for mini-batch gradient descent\n",
        "batch_size = 128\n",
        "\n",
        "# GPU\n",
        "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Embedding Size\n",
        "hidden_size = 192\n",
        "\n",
        "# Number of Transformer blocks for the (Encoder, Decoder)\n",
        "num_layers = (6, 6)\n",
        "\n",
        "# MultiheadAttention Heads\n",
        "num_heads = 8\n",
        "\n",
        "# Size of the patches\n",
        "patch_size = 8\n",
        "\n",
        "# Create model\n",
        "caption_model = VisionEncoderDecoder(image_size=image_size, channels_in=test_images.shape[1], num_emb=tokenizer.vocab_size, patch_size=patch_size, num_layers=num_layers,hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
        "\n",
        "# Initialize the optimizer with above parameters\n",
        "optimizer = optim.Adam(caption_model.parameters(), lr=learning_rate)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "td = TokenDrop(0.5)\n",
        "\n",
        "# Initialize the training loss logger\n",
        "training_loss_logger = []\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([transforms.Resize(image_size),\n",
        "                                      transforms.RandomCrop(image_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
        "                                      transforms.RandomErasing(p=0.5)])\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(image_size),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "train_dataset = datasets.CocoCaptions(root=train_image_path,\n",
        "                                      annFile=train_ann_file,\n",
        "                                      transform=train_transform,\n",
        "                                      target_transform=SampleCaption())\n",
        "\n",
        "val_dataset = datasets.CocoCaptions(root=val_image_path,\n",
        "                                    annFile=val_ann_file,\n",
        "                                    transform=transform,\n",
        "                                    target_transform=SampleCaption())\n",
        "\n",
        "# Data Load\n",
        "data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "data_loader_val = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "dataiter = next(iter(data_loader_val))\n",
        "test_images, test_captions = dataiter\n",
        "\n",
        "# Iterate over epochs\n",
        "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n",
        "    # Set the model in training mode\n",
        "    caption_model.train()\n",
        "    steps = 0\n",
        "    # Iterate over the training data loader\n",
        "    for images, captions in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Tokenize and pre-process the captions\n",
        "        tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        token_ids = tokens['input_ids'].to(device)\n",
        "        padding_mask = tokens['attention_mask'].to(device)\n",
        "        bs = token_ids.shape[0]\n",
        "\n",
        "        # Shift the input sequence to create the target sequence\n",
        "        target_ids = torch.cat((token_ids[:, 1:],\n",
        "                                torch.zeros(bs, 1, device=device).long()), 1)\n",
        "        tokens_in = td(token_ids)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # Forward pass\n",
        "            pred = caption_model(images, tokens_in, padding_mask=padding_mask)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = (loss_fn(pred.transpose(1, 2), target_ids) * padding_mask).mean()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Log the training loss\n",
        "        training_loss_logger.append(loss.item())"
      ],
      "metadata": {
        "id": "gmMeu4Ej4bmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. ëª¨ë¸ ì‹¤í–‰\n",
        "\n",
        "ëª¨ë¸ì´ ì˜ í•™ìŠµë˜ì—ˆë‹¤ë©´, ì‹¤ì œë¡œ ëª¨ë¸ì„ í†µí•´ ì´ë¯¸ì§€ì˜ ìº¡ì…˜ì„ ìƒì„±í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì„ì˜ì˜ í•œ íŒŒì¼ì„ í†µí•´, ì‹¤ì œë¡œ ìº¡ì…˜ì„ í™•ì¸í•´ë³¼ê¹Œìš”?\n",
        "\n",
        "ì—¬ê¸°ì„œëŠ” validation ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ë¡œ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ğŸ¨ Image\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=177zLaXPriug3w7MHtUcufFN5rEpbYB8A\" height = 200 width = 300>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ìœ„ì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ \"a shoe rack with some shoes and a dog sleeping on them\" ë“±ê³¼ ê°™ì€ captionì´ ìƒì„±ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ìƒìœ¼ë¡œ captioning projectë¥¼ ëª¨ë‘ ë§ˆì¹˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "3mRcZ5a6BTta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dataloader itterable object\n",
        "dataiter = next(iter(data_loader_val))\n",
        "# Sample from the itterable object\n",
        "test_images, test_captions = dataiter\n",
        "\n",
        "# Choose an index within the batch\n",
        "index = 0\n",
        "test_image = test_images[index].unsqueeze(0)\n",
        "\n",
        "# Lets visualise an entire batch of images!\n",
        "plt.figure(figsize = (3,3))\n",
        "out = torchvision.utils.make_grid(test_image, 1, normalize=True)\n",
        "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
        "print(test_captions[index])"
      ],
      "metadata": {
        "id": "mmfKIQs0-hnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}